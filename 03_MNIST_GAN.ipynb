{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reference\n",
    "    - https://github.com/dreamgonfly/GAN-tutorial/blob/master/GAN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as utils\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')\n",
    "\n",
    "# 0~1로 standardize -> G에서 tanh 써야함\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # change data as tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])]) # 0~1의 pixel값(grey scale)을 -1~1로 바꾼다\n",
    "\n",
    "# MNIST dataset\n",
    "train_data = datasets.MNIST(root='data/', train=True, transform=transform, download=True)\n",
    "test_data  = datasets.MNIST(root='data/', train=False, transform=transform, download=True)\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "dim_z = 100\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "test_data_loader  = torch.utils.data.DataLoader(test_data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_mini_batch_img, example_mini_batch_label  = next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input notse generator for Generator\n",
    "def input_noise_generator(batch_size, dim):\n",
    "    return torch.randn(batch_size, dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_generated_mnist(x, epoch):\n",
    "    # sample one\n",
    "    sample_index = random.randint(0,batch_size-1)\n",
    "    \n",
    "    # reshape\n",
    "    x_reshape = x.data[sample_index].numpy().squeeze()\n",
    "    min_x = x_reshape.min()\n",
    "    max_x = x_reshape.max()\n",
    "    \n",
    "    # normalize\n",
    "    x_normalize = (x_reshape - min_x)/(max_x - min_x)\n",
    "    \n",
    "    # save image\n",
    "    plt.imsave(f'img/G_epoch_{epoch}.png', x_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(dim_z, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 28*28),\n",
    "            nn.Tanh() #since normalized to -1~1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # to make visualization easy, change dimension\n",
    "        return self.model(x).view(-1,1,28,28)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28*28, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # reshape dim (batch_size,1,28,28) ->( batch_size,28*28)\n",
    "        x = x.view(-1, 28*28)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make instance and allocate device\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "\n",
    "if is_cuda:\n",
    "    G.cuda()\n",
    "    D.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "G_optimizer = optim.SGD(G.parameters(), lr = 1e-3, momentum=0.8)\n",
    "D_optimizer = optim.SGD(G.parameters(), lr = 1e-3, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------epoch 0----------\n",
      "Generator loss: 0.7086819410324097\n",
      "Discriminator loss: 1.3958896398544312\n",
      "----------epoch 1----------\n",
      "Generator loss: 0.7062671184539795\n",
      "Discriminator loss: 1.3908379077911377\n",
      "----------epoch 2----------\n",
      "Generator loss: 0.7030912041664124\n",
      "Discriminator loss: 1.3893771171569824\n",
      "----------epoch 3----------\n",
      "Generator loss: 0.7001062631607056\n",
      "Discriminator loss: 1.3905487060546875\n",
      "----------epoch 4----------\n",
      "Generator loss: 0.6956173777580261\n",
      "Discriminator loss: 1.4048317670822144\n",
      "----------epoch 5----------\n",
      "Generator loss: 0.6904615163803101\n",
      "Discriminator loss: 1.4054063558578491\n",
      "----------epoch 6----------\n",
      "Generator loss: 0.6849323511123657\n",
      "Discriminator loss: 1.410360336303711\n",
      "----------epoch 7----------\n",
      "Generator loss: 0.6752581596374512\n",
      "Discriminator loss: 1.4230632781982422\n",
      "----------epoch 8----------\n",
      "Generator loss: 0.6600431799888611\n",
      "Discriminator loss: 1.4275026321411133\n",
      "----------epoch 9----------\n",
      "Generator loss: 0.648216187953949\n",
      "Discriminator loss: 1.4447320699691772\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    D_loss = []\n",
    "    G_loss = []\n",
    "    \n",
    "    for mini_batch_img, mini_batch_label in train_data_loader:\n",
    "        D_loss_batch = []\n",
    "        G_loss_batch = []\n",
    "        \n",
    "        # convert data as tensor\n",
    "        mini_batch = Variable(mini_batch_img)\n",
    "        \n",
    "        # make label\n",
    "        target_real = Variable(torch.ones(batch_size, 1))\n",
    "        target_fake = Variable(torch.zeros(batch_size, 1))\n",
    "        \n",
    "        # push all tensor to cuda\n",
    "        if is_cuda:\n",
    "            mini_batch = mini_batch.cuda()\n",
    "            target_real = target_real.cuda()\n",
    "            target_fake = target_fake.cuda()\n",
    "        \n",
    "        # Generate fake image\n",
    "        random_noise = input_noise_generator(batch_size, dim_z)\n",
    "        fake_batch = G(random_noise) \n",
    "        \n",
    "        # Discriminate real and fake images\n",
    "        D_result_real = D(mini_batch)\n",
    "        D_result_fake = D(fake_batch)\n",
    "        \n",
    "        # calculate loss for discriminator\n",
    "        D_loss_real = criterion(D_result_real, target_real)\n",
    "        D_loss_fake = criterion(D_result_fake, target_fake)\n",
    "        D_loss_total = D_loss_real + D_loss_fake\n",
    "        \n",
    "        # backprop discriminator\n",
    "        D.zero_grad()\n",
    "        D_loss_total.backward()\n",
    "        D_optimizer.step()\n",
    "        \n",
    "        # calculate loss for generator\n",
    "        ## loss gets lower if discriminator was fooled\n",
    "        random_noise = input_noise_generator(batch_size, dim_z)\n",
    "        fake_batch = G(random_noise) \n",
    "        D_result_fake = D(fake_batch)\n",
    "        G_loss_total = criterion(D_result_fake, target_real)\n",
    "        \n",
    "        # backprop generator\n",
    "        G.zero_grad()\n",
    "        G_loss_total.backward()\n",
    "        G_optimizer.step()\n",
    "        \n",
    "        G_loss_batch.append(G_loss_total.data.item())\n",
    "        D_loss_batch.append(D_loss_total.data.item())\n",
    "        \n",
    "    # print error\n",
    "    print(f\"----------epoch {epoch}----------\")\n",
    "    print(f\"Generator loss: {np.mean(G_loss_batch)}\")\n",
    "    print(f\"Discriminator loss: {np.mean(D_loss_batch)}\")\n",
    "    \n",
    "    # sample out\n",
    "    visualize_generated_mnist(fake_batch, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py36_torch",
   "language": "python",
   "name": "py36_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
