{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "dtype = torch.FloatTensor\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in/out data (might be placeholder)\n",
    "# randomly generate in/out data with normal distribution\n",
    "# numpy와 기본적으로 비슷하게 생겼다. pythonic\n",
    "# dtype 나중에 꼬이면 골치아프니까 꼭 미리 지정해주자\n",
    "# dim(x) = (N, D_in)\n",
    "x = torch.randn(N, D_in).type(dtype) \n",
    "y = torch.randn(N, D_out).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "# initialize weight matrix\n",
    "W1 = torch.randn(D_in, H).type(dtype)\n",
    "W2 = torch.randn(H, D_out).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    # matrix multiplication => tensor_1.mm(tensor_2)\n",
    "    hidden_layer = x.mm(W1)\n",
    "    \n",
    "    # relu activation(customize with torch.clamp)\n",
    "    ## torch.clamp(input, min, max) => min_max 사이면 input 그대로, 그 외엔 min/max return\n",
    "    hidden_layer_relu = hidden_layer.clamp(min=0)\n",
    "    y_pred = hidden_layer_relu.mm(W2)\n",
    "    \n",
    "    # calculate loss function\n",
    "    # euclidean distance(**2 써줘도 가능)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(f\"iter {i} loss: {loss}\")\n",
    "    \n",
    "    # calculate gradient\n",
    "    # 첫 예제니까 gradient 손 계산\n",
    "    grad_y_pred = 2.0*(y_pred - y)\n",
    "    grad_W2 = hidden_layer_relu.t().mm(grad_y_pred)\n",
    "    grad_hidden_layer_relu = grad_y_pred.mm(W2.t())\n",
    "    grad_hidden_layer = grad_hidden_layer_relu.clone()\n",
    "    grad_hidden_layer[hidden_layer < 0] = 0 #이런 식으로 indexing도 가능\n",
    "    grad_W1 = x.t().mm(grad_hidden_layer)\n",
    "    \n",
    "    W1 -= learning_rate * grad_W1\n",
    "    W2 -= learning_rate * grad_W2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference loss\n",
    "((x.mm(W1).clamp(min=0).mm(W2) - y) ** 2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " gradient 손 계산 탈출 - Variable 이용\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "dtype = torch.FloatTensor\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in/out data (might be placeholder)\n",
    "# randomly generate in/out data with normal distribution\n",
    "# Variable instance는 자동 gradient update 해줌\n",
    "# requires_grad=False 하면 업뎃 제외\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "# initialize weight matrix\n",
    "W1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "W2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    y_pred = x.mm(W1).clamp(min=0).mm(W2)\n",
    "    \n",
    "    # **2 해도 되지만 sum()까지 method chain 해주기 위해\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(f\"iter {i} loss: {loss}\")\n",
    "    \n",
    "    # gradient를 자동으로 계산해준다\n",
    "    # Variable instance의 .grad field를 이용해서 gradient 값 접근 가능하다.\n",
    "    loss.backward()\n",
    "    \n",
    "    # w1.data: tensor/ w1.grad: Variable/ w1.grad.data: tensor\n",
    "    # tensor에 대해서 연산을 취해준다.\n",
    "    # Variable은 값에 직접적인 접근이 안되는 instance로 일단 생각해두자\n",
    "    # 프린드해보면, Variable은 requires_grad 까지 출력되는 Variable이다.\n",
    "    W1.data -= learning_rate * W1.grad.data\n",
    "    W2.data -= learning_rate * W2.grad.data\n",
    "    \n",
    "    # gradient descent가 끝났으니, gradient를 0으로 만들어준다.\n",
    "    W1.grad.data.zero_()\n",
    "    W2.grad.data.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference loss\n",
    "((x.mm(W1).clamp(min=0).mm(W2) - y) ** 2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. define new autograd function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    autograd.Function 상속, 나만의 autograd 정의\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        ctx는 autograd.Fucntion에 미리 정의된 field\n",
    "        backpropagation을 위한 정보를 저장하는 context object\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        input 값에 대한 loss의 변화도(gradient)를 return한다.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "dtype = torch.FloatTensor\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "W1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "W2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    # 상속받은 torch.autograd.Function의 apply method 적용(정확히 말하면 그 method가 적용된 instance인듯?\n",
    "    relu = CustomReLU.apply\n",
    "    \n",
    "    # 위에서 직접 relu를 계산해준 것과 달리, 이젠 relu instance로 편하게 연산\n",
    "    y_pred = relu(x.mm(W1)).mm(W2)\n",
    "    \n",
    "    # **2 해도 되지만 sum()까지 method chain 해주기 위해\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(f\"iter {i} loss: {loss}\")\n",
    "    \n",
    "    # gradient를 자동으로 계산해준다\n",
    "    # Variable instance의 .grad field를 이용해서 gradient 값 접근 가능하다.\n",
    "    loss.backward()\n",
    "    \n",
    "    # w1.data: tensor/ w1.grad: Variable/ w1.grad.data: tensor\n",
    "    # tensor에 대해서 연산을 취해준다.\n",
    "    # Variable은 값에 직접적인 접근이 안되는 instance로 일단 생각해두자\n",
    "    # 프린드해보면, Variable은 requires_grad 까지 출력되는 Variable이다.\n",
    "    W1.data -= learning_rate * W1.grad.data\n",
    "    W2.data -= learning_rate * W2.grad.data\n",
    "    \n",
    "    # gradient descent가 끝났으니, gradient를 0으로 만들어준다.\n",
    "    W1.grad.data.zero_()\n",
    "    W2.grad.data.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference loss\n",
    "((relu(x.mm(W1)).mm(W2) - y) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_torch",
   "language": "python",
   "name": "py36_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
