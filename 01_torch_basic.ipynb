{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "dtype = torch.FloatTensor\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in/out data (might be placeholder)\n",
    "# randomly generate in/out data with normal distribution\n",
    "# numpy와 기본적으로 비슷하게 생겼다. pythonic\n",
    "# dtype 나중에 꼬이면 골치아프니까 꼭 미리 지정해주자\n",
    "# dim(x) = (N, D_in)\n",
    "x = torch.randn(N, D_in).type(dtype) \n",
    "y = torch.randn(N, D_out).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "# initialize weight matrix\n",
    "W1 = torch.randn(D_in, H).type(dtype)\n",
    "W2 = torch.randn(H, D_out).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    # matrix multiplication => tensor_1.mm(tensor_2)\n",
    "    hidden_layer = x.mm(W1)\n",
    "    \n",
    "    # relu activation(customize with torch.clamp)\n",
    "    ## torch.clamp(input, min, max) => min_max 사이면 input 그대로, 그 외엔 min/max return\n",
    "    hidden_layer_relu = hidden_layer.clamp(min=0)\n",
    "    y_pred = hidden_layer_relu.mm(W2)\n",
    "    \n",
    "    # calculate loss function\n",
    "    # euclidean distance(**2 써줘도 가능)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(f\"iter {i} loss: {loss}\")\n",
    "    \n",
    "    # calculate gradient\n",
    "    # 첫 예제니까 gradient 손 계산\n",
    "    grad_y_pred = 2.0*(y_pred - y)\n",
    "    grad_W2 = hidden_layer_relu.t().mm(grad_y_pred)\n",
    "    grad_hidden_layer_relu = grad_y_pred.mm(W2.t())\n",
    "    grad_hidden_layer = grad_hidden_layer_relu.clone()\n",
    "    grad_hidden_layer[hidden_layer < 0] = 0 #이런 식으로 indexing도 가능\n",
    "    grad_W1 = x.t().mm(grad_hidden_layer)\n",
    "    \n",
    "    W1 -= learning_rate * grad_W1\n",
    "    W2 -= learning_rate * grad_W2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference loss\n",
    "((x.mm(W1).clamp(min=0).mm(W2) - y) ** 2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " gradient 손 계산 탈출 - Variable 이용\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "dtype = torch.FloatTensor\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in/out data (might be placeholder)\n",
    "# randomly generate in/out data with normal distribution\n",
    "# Variable instance는 자동 gradient update 해줌\n",
    "# requires_grad=False 하면 업뎃 제외\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "# initialize weight matrix\n",
    "W1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "W2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    y_pred = x.mm(W1).clamp(min=0).mm(W2)\n",
    "    \n",
    "    # **2 해도 되지만 sum()까지 method chain 해주기 위해\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(f\"iter {i} loss: {loss}\")\n",
    "    \n",
    "    # gradient를 자동으로 계산해준다\n",
    "    # Variable instance의 .grad field를 이용해서 gradient 값 접근 가능하다.\n",
    "    loss.backward()\n",
    "    \n",
    "    # w1.data: tensor/ w1.grad: Variable/ w1.grad.data: tensor\n",
    "    # tensor에 대해서 연산을 취해준다.\n",
    "    # Variable은 값에 직접적인 접근이 안되는 instance로 일단 생각해두자\n",
    "    # 프린드해보면, Variable은 requires_grad 까지 출력되는 Variable이다.\n",
    "    W1.data -= learning_rate * W1.grad.data\n",
    "    W2.data -= learning_rate * W2.grad.data\n",
    "    \n",
    "    # gradient descent가 끝났으니, gradient를 0으로 만들어준다.\n",
    "    W1.grad.data.zero_()\n",
    "    W2.grad.data.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference loss\n",
    "((x.mm(W1).clamp(min=0).mm(W2) - y) ** 2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. define new autograd function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    autograd.Function 상속, 나만의 autograd 정의\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        ctx는 autograd.Fucntion에 미리 정의된 field\n",
    "        backpropagation을 위한 정보를 저장하는 context object\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        input 값에 대한 loss의 변화도(gradient)를 return한다.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "dtype = torch.FloatTensor\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "W1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "W2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    # 상속받은 torch.autograd.Function의 apply method 적용(정확히 말하면 그 method가 적용된 instance인듯?\n",
    "    relu = CustomReLU.apply\n",
    "    \n",
    "    # 위에서 직접 relu를 계산해준 것과 달리, 이젠 relu instance로 편하게 연산\n",
    "    y_pred = relu(x.mm(W1)).mm(W2)\n",
    "    \n",
    "    # **2 해도 되지만 sum()까지 method chain 해주기 위해\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(f\"iter {i} loss: {loss}\")\n",
    "    \n",
    "    # gradient를 자동으로 계산해준다\n",
    "    # Variable instance의 .grad field를 이용해서 gradient 값 접근 가능하다.\n",
    "    loss.backward()\n",
    "    \n",
    "    # w1.data: tensor/ w1.grad: Variable/ w1.grad.data: tensor\n",
    "    # tensor에 대해서 연산을 취해준다.\n",
    "    # Variable은 값에 직접적인 접근이 안되는 instance로 일단 생각해두자\n",
    "    # 프린드해보면, Variable은 requires_grad 까지 출력되는 Variable이다.\n",
    "    W1.data -= learning_rate * W1.grad.data\n",
    "    W2.data -= learning_rate * W2.grad.data\n",
    "    \n",
    "    # gradient descent가 끝났으니, gradient를 0으로 만들어준다.\n",
    "    W1.grad.data.zero_()\n",
    "    W2.grad.data.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference loss\n",
    "((relu(x.mm(W1)).mm(W2) - y) ** 2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Static Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensorflow는 연산 그래프를 한 번 정의한 후 동일한 그래프를 계속해서 실행한다. -> 정적\n",
    "    - 미리 정의되어있는 것이기 때문에 최적화에 좋다.\n",
    "    - 여러 개의 gpu에 그래프를 배포할 수도 있다.\n",
    "    - 반복 loop는 그래프 안에서 정의되어야한다.\n",
    "- pytorch는 각각의 순전파 단계에서 새로운 연산 그래프를 정의한다. -> 동적\n",
    "    - 그래프를 즉석(on-the-fly)에서 작성하기 때문에 일반적인 명령형으로 각각의 입력에 따라 다른 계산을 수행할 수 있다.\n",
    "- tensorflow 코드 예제이므로 넘어가도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. nn 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 매번 autograd를 이용하는 방식으로 직접 SGD를 정의할 수 없다. 미리 정의된 모듈을 활용하자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "dtype = torch.FloatTensor\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed forward nn을 그냥 layer 연산 순서대로 나열\n",
    "## ReLU같은 activation function은 그냥 중간에 method만 써준다.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H,D_out)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞서 사용하던 MSE loss를 미리 정의된 것으로 쉽게 사용할 수 있다.\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    # model이라는 Sequential instance에 x를 넣어주기만 하면 된다. 마치 method처럼 호출이 가능하다.\n",
    "    # y_pred 역시 반환되는 variable이다.\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # 마찬가지로 method처럼 호출이 가능하다. loss 역시 variabledㅣ다.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(f\"iter {i} loss: {loss.data}\")\n",
    "    \n",
    "    # gradient를 0으로 세팅한다.\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # model의 모든 학습 가능한 변수에 대하여 gradient 계산\n",
    "    # nn.Sequential 안에 정의한 모든 layer의 변수는 기본적으로 requires_grad=True로 세팅 되는 것으로 보인다.\n",
    "    loss.backward()\n",
    "    \n",
    "    # param은 (당연한 말이겠지만) Variable 객체일 것이므로 .grad.data로 tenser에 접근 가능하다.\n",
    "    # model.parameters()는 generator 객체이다. 즉, itertools로 접근 가능하다.\n",
    "    # [*model.parameters()]로 까보면 requires_grad = True인 Variable로 선언된 것을 확인 가능하다.\n",
    "    # Variable name은 따로 정의되지 않은 것으로 보이는데, 이것은 지정해줘야할 것 같다.\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad.data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_torch",
   "language": "python",
   "name": "py36_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
